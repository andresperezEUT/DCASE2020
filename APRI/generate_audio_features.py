"""
generate_audio_features.py

For each audio file, the script generates a set of audio features.

The output values are stored in a folder `oracle_mono_signals/audio_features` within `params['dataset_dir']`.
Inside it, a folder is created for each event class, and a single file in created for each event
keeping the name (number) of the original audio file.

"""
from baseline import parameter
import essentia
import essentia.standard as es
import os
import numpy as np
from baseline.cls_feature_class import create_folder
from APRI.utils import get_class_name_dict
from APRI.get_audio_features import *

def get_feature_list():
    return [
         'lowlevel.barkbands.dmean',
         'lowlevel.barkbands.mean',
         'lowlevel.barkbands.var',
         'lowlevel.erbbands.dmean',
         'lowlevel.erbbands.mean',
         'lowlevel.erbbands.var',
         'lowlevel.gfcc.mean',
         'lowlevel.melbands.dmean',
         'lowlevel.melbands.mean',
         'lowlevel.melbands.var',
         'lowlevel.mfcc.mean',
         'lowlevel.spectral_contrast_coeffs.dmean',
         'lowlevel.spectral_contrast_coeffs.mean',
         'lowlevel.spectral_contrast_coeffs.var',
         'lowlevel.spectral_contrast_valleys.dmean',
         'lowlevel.spectral_contrast_valleys.mean',
         'lowlevel.spectral_contrast_valleys.var',
         'rhythm.beats_loudness_band_ratio.dmean',
         'rhythm.beats_loudness_band_ratio.mean',
         'rhythm.beats_loudness_band_ratio.var',
         'tonal.hpcp.dmean',
         'tonal.hpcp.mean',
         'tonal.hpcp.var',
         'tonal.chords_histogram',
         'tonal.thpcp',
         'lowlevel.pitch_salience.dmean',
         'lowlevel.pitch_salience.mean',
         'lowlevel.pitch_salience.var',
         'lowlevel.silence_rate_20dB.dmean',
         'lowlevel.silence_rate_20dB.mean',
         'lowlevel.silence_rate_20dB.var',
         'lowlevel.silence_rate_30dB.dmean',
         'lowlevel.silence_rate_30dB.mean',
         'lowlevel.silence_rate_30dB.var',
         'lowlevel.silence_rate_60dB.dmean',
         'lowlevel.silence_rate_60dB.mean',
         'lowlevel.silence_rate_60dB.var'
    ]


params = parameter.get_params()
event_type= get_class_name_dict().values()
data_folder_path = os.path.join(params['dataset_dir'], 'oracle_mono_signals_beam_all/') # path to audios
audio_features_output_path= os.path.join(data_folder_path,'audio_features_optimized/')
get_column_labels= True
mode='optimized'


# Compute each audio_file generated by "generate_audio_from_annotations.py" and generates audio_features using MusicExtractor
if mode=='musicextractor':
    if get_column_labels:
        h=0
    else:
        h=1
    for event in event_type:
        create_folder(os.path.join(audio_features_output_path,event))
        audio_path= os.path.join(data_folder_path,event) #path to file
        for audio in os.scandir(audio_path):
            print("Extracting features from ",event+' ' + audio.name)
            # TODO: set verbose parameter to false
            # TODO: add parameter 'analysisSampleRate=24000' in order to speed up process
            # TODO: try out with 'chromaprintCompute=True'
            features, features_frames = es.MusicExtractor(
                                                  chromaprintCompute=True,
                                                  analysisSampleRate=32000,
                                                  lowlevelFrameSize=4096,
                                                  lowlevelHopSize=2048,
                                                  tonalFrameSize=4096,
                                                  tonalHopSize=2048,
                                                  rhythmStats = ["mean", "var", "dmean"],
                                                  lowlevelStats = ["mean", "var", "dmean"],
                                                 )(audio.path)
            feature_list=get_feature_list()
            audio_features=[]
            column_labels = []
            print("Converting to array...")

            for feature in feature_list:
                x=features[feature]

                if type(x) is float:
                    x=np.array(x)
                    y=[x]
                else:
                    y=x.tolist()
                c=0
                if h==0:
                    print('Obtaining column labels...')
                    for i in range(len(y)):
                        c+=1
                        z=feature+str(c)
                        column_labels.append(z)
                    np.save(os.path.join(audio_features_output_path,'column_labels.npy'), column_labels)
                audio_features=audio_features+y
            audio_features=np.array(audio_features)
            h=1
            # Save arrays
            file_name = os.path.splitext(audio.name)[0]
            print("Saving file ",event+file_name+'.npy')
            np.save(os.path.join(audio_features_output_path, event, file_name+'.npy'), audio_features)
else:
    for event in event_type:
        create_folder(os.path.join(audio_features_output_path,event))
        audio_path= os.path.join(data_folder_path,event) #path to file
        for audio in os.scandir(audio_path):
            print("Extracting features from ", event + ' ' + audio.name)
            audio_features, column_labels = compute_audio_features(audio, options)
            file_name = os.path.splitext(audio.name)[0]
            np.save(os.path.join(audio_features_output_path, 'column_labels.npy'), column_labels)
            np.save(os.path.join(audio_features_output_path, event,file_name+ '.npy'), audio_features)
